{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootF = 'D:/Data/Paper defocus/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "# from data import Dataset\n",
    "\n",
    "from shared.save_load import load_obj\n",
    "from shared.functions import fit_gaussian, get_ellipse_parameters, plot_scale_bar, load_image_as_on_MEA, corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = 20211129\n",
    "\n",
    "if exp == 20211208:\n",
    "    trial = 67\n",
    "    run_nb = 30\n",
    "    n_cells = 29\n",
    "\n",
    "elif exp == 20211129:\n",
    "    trial = 20\n",
    "    run_nb = 65\n",
    "    n_cells = 21\n",
    "\n",
    "elif exp == 20211207:\n",
    "    trial = 37\n",
    "    run_nb = 58\n",
    "    n_cells = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = rootF + \"/Modelling/exp\" + str(exp) + '/cnn_{}_{}cells_tr{}/run_0{}/'.format(exp, n_cells, trial, run_nb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_observations = np.load(os.path.join(results_folder, 'test_observations.npy'))\n",
    "test_predictions = np.load(os.path.join(results_folder, 'test_predictions.npy'))\n",
    "\n",
    "explained_var_test_set = np.empty(n_cells)\n",
    "\n",
    "test_y = test_observations\n",
    "pred_y = np.mean(test_predictions, axis=0) # mean over the 30 repetitions\n",
    "nb_conditions, nb_cells = test_y.shape[1:]\n",
    "\n",
    "# Measure noise-corrected correlation \n",
    "even_test_y = np.mean(test_y[0::2, :, :], axis=0)  # mean over even repetitions\n",
    "odd_test_y = np.mean(test_y[1::2, :, :], axis=0)  # mean over odd repetitions\n",
    "\n",
    "for cell_nb in range(0, n_cells):\n",
    "    reliability = corrcoef(even_test_y[:, cell_nb], odd_test_y[:, cell_nb])\n",
    "    accuracy_odd = corrcoef(pred_y[:, cell_nb], odd_test_y[:, cell_nb])\n",
    "    accuracy_even = corrcoef(pred_y[:, cell_nb], even_test_y[:, cell_nb])\n",
    "    noise_corrected_correlation = 0.5*(accuracy_odd + accuracy_even)/np.sqrt(reliability)\n",
    "    explained_var_test_set[cell_nb] = noise_corrected_correlation**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 21)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blurred images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: (4, 2, 9, 4, 21)\n",
      "(21, 4, 2, 9, 4)\n",
      "[  1  96 140 141 200 234 293 314 347 390 396 401 406 416 417 424 437 508\n",
      " 558 583 614]\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "pred_data = np.load(rootF + \"/Modelling/exp\" + str(exp) + \"/CNN_predictions_4_images_\" + str(exp) + \"_\" + str(n_cells) \\\n",
    "         + \"cells_tr\" + str(trial) + \"_run\" + str(run_nb) + \".npz\")\n",
    "\n",
    "predictions = pred_data['predictions']\n",
    "print ('predictions:', predictions.shape)\n",
    "\n",
    "# Data\n",
    "Imgs = ['1','2','3','5']\n",
    "pupDiams = ['1.40','2.00']\n",
    "sources = ['1','2','3','4'] \n",
    "sources_name = ['0','10','20','30']\n",
    "dFs = ['+40','+30','+20','+10','+00','-10','-20','-30','-40'] \n",
    "\n",
    "Nimgs = len(Imgs)\n",
    "NpupDiams = len(pupDiams)\n",
    "Nsources = len(sources)\n",
    "NdFs = len(dFs)\n",
    "\n",
    "data_path = rootF + '/Modelling/exp' + str(exp) + '/'\n",
    "\n",
    "blur_data = np.load(data_path + '{}_spike_counts_CNN_{}cells_tr{}_run{}.npz'.format(exp, n_cells, trial, run_nb))\n",
    "spike_counts_mean = blur_data['spike_count_mean']\n",
    "spike_counts_std = blur_data['spike_count_std']\n",
    "spike_counts = blur_data['spike_count']\n",
    "clusters = blur_data['clusters']\n",
    "\n",
    "Ncells = len(clusters)\n",
    "\n",
    "print (np.shape(spike_counts_mean))\n",
    "print (clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9956432626765868\n",
      "96 0.8846743833433172\n",
      "140 0.9564957168433013\n",
      "141 0.9822562709818055\n",
      "200 0.9812789347258021\n",
      "234 0.9632079605480304\n",
      "293 0.9868583952268931\n",
      "314 0.9532318358328034\n",
      "347 0.9883541211943745\n",
      "390 0.9974977180909501\n",
      "396 0.9142506637428331\n",
      "401 0.8280226143145039\n",
      "406 0.9853026504609358\n",
      "416 0.9756573120288461\n",
      "417 0.9803955486483138\n",
      "424 0.9964544855782862\n",
      "437 0.9734644246616077\n",
      "508 0.9917966471521908\n",
      "558 0.355796046718513\n",
      "583 0.9672665689785307\n",
      "614 0.7335438643187043\n"
     ]
    }
   ],
   "source": [
    "explained_var_blur_set = np.zeros(n_cells)\n",
    "\n",
    "for iCell, cell in enumerate(clusters):\n",
    "    test_y = spike_counts[iCell].reshape((288,25))[:,:-1]\n",
    "    pred_y = predictions[:,:,:,:,iCell].reshape((288))\n",
    "    nb_conditions = 25\n",
    "\n",
    "    # Measure noise-corrected correlation \n",
    "    even_test_y = np.mean(test_y[:, 0::2], axis=1)  # mean over even repetitions\n",
    "    odd_test_y = np.mean(test_y[:, 1::2], axis=1)  # mean over odd repetitions\n",
    "\n",
    "    reliability = corrcoef(even_test_y, odd_test_y)\n",
    "    accuracy_odd = corrcoef(pred_y, odd_test_y)\n",
    "    accuracy_even = corrcoef(pred_y, even_test_y)\n",
    "    noise_corrected_correlation = 0.5*(accuracy_odd + accuracy_even)/np.sqrt(reliability)\n",
    "    explained_var_blur_set[iCell] = noise_corrected_correlation**2\n",
    "    print (cell, noise_corrected_correlation**2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Save explained variance\n",
    "np.savez(rootF + '/Modelling/exp{}/CNN_trial{}_explained_variance_blur.npz'.format(exp, trial), \\\n",
    "                cells=clusters, explained_variance = explained_var_blur_set)\n",
    "\n",
    "# Save explained variance\n",
    "np.savez(rootF + '/Modelling/exp{}/CNN_trial{}_explained_variance_test.npz'.format(exp, trial), \\\n",
    "                cells=clusters, explained_variance = explained_var_test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
